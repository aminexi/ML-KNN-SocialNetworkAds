# -*- coding: utf-8 -*-
"""KNN PROJECT

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/169YTtv9RaFOOwMYD3TA8DQJrEjNdx6z9

# **Importation des bibliothèques**
"""

import numpy as np
import gdown
import matplotlib.pyplot as plt
import pandas as pd

"""# **Importation de l'ensemble de données**"""

file_id = "17dTsSuTml92HDa1coTra2RHDChqxdyqy"
url = f"https://drive.google.com/uc?id={file_id}"

output = "Social_Network_Ads.csv"
gdown.download(url, output, quiet=False)

dataset = pd.read_csv('Social_Network_Ads.csv')

"""# **Affichage des premières entrées de l'ensemble de données**"""

print(dataset.head())

"""# **Les caractéristiques suivantes seront considérées comme les variables indépendantes (variables d’entrée)**

*1) Salaire estimé*

*2) Sexe*

"""

from sklearn.preprocessing import LabelEncoder
dataset['Gender']= LabelEncoder().fit_transform(dataset['Gender'].astype(str))

"""# **Affectation des variables d’entrée:**

> Add blockquote


"""

X = dataset.iloc[:, [1, 3]].values

"""# **Affectation de la variable de sortie :**

"""

Y = dataset.iloc[:, 4].values

"""# **Séparation de l'ensemble de données en un ensemble d’entrainement et un ensemble de test.**"""

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size = 0.25,
random_state = 0)

"""# **Nous allons utiliser Feature Scaling pour normaliser la data (Variable de même échelle) et améliorer la performance du modèle.**

"""

from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)

"""# **Entraînement du modèle**
Nous devons donc importer la bibliothèque scikit.neighbours et y importer le
classificateur KNN.

"""

from sklearn.neighbors import KNeighborsClassifier
classifier = KNeighborsClassifier()
classifier.fit(X_train, y_train)

"""# **Prédire les résultats de l'ensemble de test**

"""

y_pred = classifier.predict(X_test)
# Displaying out the predicted values
print(y_pred)
# Maintenant, pour calculer la précision de notre modèle...
c=0
for i in range(0,len(y_pred)):
  if(y_pred[i]==y_test[i]):
    c=c+1
accuracy=c/len(y_pred)
print("Accuracy is")
print(accuracy)

"""# **Maintenant, la section suivante est la visualisation des données, qui nous aide à visualiser la précision et les erreurs de notre modèle.**"""

from sklearn.metrics import confusion_matrix
cm = confusion_matrix(y_test, y_pred)
matrice_conf=pd.DataFrame(cm,index=['positive','négative'],columns=['positive','negative'])

import seaborn as sns
import matplotlib.pyplot as plt

sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", xticklabels=['positive','negative'], yticklabels=['positive','négative'])
plt.xlabel('Prédictions')
plt.ylabel('Vraies valeurs')
plt.show()

"""# **l’entrainement en utilisant juste les variables d’entrée suivantes : Gender, salaire et l’Age.**

Affectation des variables d’entrée:
"""

X = dataset.iloc[:, [1,2, 3]].values

"""Affectation de la variable de sortie :"""

Y = dataset.iloc[:, 4].values

"""Séparation de l'ensemble de données en un ensemble d’entrainement et un ensemble de test."""

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size = 0.25,
random_state = 0)

"""Nous allons utiliser Feature Scaling pour normaliser la data (Variable de même échelle) et améliorer la performance du modèle."""

from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)

"""Entraînement du modèle

Nous devons donc importer la bibliothèque scikit.neighbours et y importer le classificateur KNN.
"""

from sklearn.neighbors import KNeighborsClassifier
classifier = KNeighborsClassifier()
classifier.fit(X_train, y_train)

"""Prédire les résultats de l'ensemble de test"""

y_pred = classifier.predict(X_test)
# Displaying out the predicted values
print(y_pred)
# Maintenant, pour calculer la précision de notre modèle...
c=0
for i in range(0,len(y_pred)):
  if(y_pred[i]==y_test[i]):
    c=c+1
accuracy=c/len(y_pred)
print("Accuracy is")
print(accuracy)

"""Maintenant, la section suivante est la visualisation des données, qui nous aide à visualiser la précision et les erreurs de notre modèle."""

from sklearn.metrics import confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

cm = confusion_matrix(y_test, y_pred)
matrice_conf=pd.DataFrame(cm,index=['positive','négative'],columns=['positive','negative'])
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", xticklabels=['positive','negative'], yticklabels=['positive','négative'])
plt.xlabel('Prédictions')
plt.ylabel('Vraies valeurs')
plt.show()

"""# **Les faits**

*Avec la variable Age :*

Prédictions : [0 0 0 0 ... 1]

Accuracy = 0.93 → 93% des utilisateurs ont été correctement classés.

*Sans la variable Age :*

Prédictions : [1 1 0 0 ... 1]

Accuracy = 0.75 → seulement 75% des utilisateurs ont été correctement classés.

# **Interprétation**

-- L’âge est une feature très importante pour prédire si un utilisateur achète ou non le produit :

Quand on l’inclut, le modèle KNN a beaucoup plus d’informations pour distinguer les acheteurs des non-acheteurs.

-- L’accuracy passe de 75% à 93%, ce qui est une amélioration significative.

Sans l’âge, le modèle se base uniquement sur :

*Gender (0 ou 1)*

*EstimatedSalary*

Et apparemment, ces deux features seules ne suffisent pas pour bien séparer les classes, donc il se trompe plus souvent → accuracy plus faible.

-- Pourquoi KNN améliore avec Age ?

KNN classe un nouvel utilisateur selon ses voisins les plus proches.

L’âge aide à mieux trouver ces voisins « pertinents » pour la prédiction.

Sans Age, les distances sont moins représentatives → erreurs plus nombreuses.

# ***Conclusion***
La feature Age est très discriminante dans notre dataset pour prédire l’achat.

Cela montre l’importance de bien choisir les features pour un modèle de machine learning.
"""